{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a Ham Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "corpus_path = 'C:/Users/Rik/Documents/corpus/'\n",
    "file_path = corpus_path + 'enron1/ham/0007.1999-12-14.farmer.ham.txt'\n",
    "with open(file_path, 'r') as infile:\n",
    "    ham_sample = infile.read()\n",
    "    print(ham_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a Spam Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: stacey automated system generating 8 k per week parallelogram\n",
      "people are\n",
      "getting rich using this system ! now it ' s your\n",
      "turn !\n",
      "we ' ve\n",
      "cracked the code and will show you . . . .\n",
      "this is the\n",
      "only system that does everything for you , so you can make\n",
      "money\n",
      ". . . . . . . .\n",
      "because your\n",
      "success is . . . completely automated !\n",
      "let me show\n",
      "you how !\n",
      "click\n",
      "here\n",
      "to opt out click here % random _ text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = corpus_path + 'enron1/spam/0058.2003-12-21.GP.spam.txt'\n",
    "with open(file_path, 'r') as infile:\n",
    "    spam_sample = infile.read()\n",
    "    print(spam_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in ham and spam email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails, labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFilesForFolder(folder_path, is_spam):\n",
    "    file_path = corpus_path + 'enron1/' + folder_path\n",
    "    for filename in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"ISO-8859-1\") as infile:\n",
    "            emails.append(infile.read())\n",
    "            labels.append(is_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "1500\n",
      "5172\n",
      "5172\n"
     ]
    }
   ],
   "source": [
    "readFilesForFolder('spam', 1)\n",
    "print(len(emails))\n",
    "print(len(labels))\n",
    "readFilesForFolder('ham', 0)\n",
    "print(len(emails))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess and clean raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def is_letter_only(word):\n",
    "    return word.isalpha()\n",
    "all_names = set(names.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def clean_text(docs):\n",
    "    docs_cleaned = []\n",
    "    for doc in docs:\n",
    "        doc = doc.lower()\n",
    "        doc_cleaned = ' '.join(lemmatizer.lemmatize(word) for word in doc.split() if is_letter_only(word))\n",
    "        docs_cleaned.append(doc_cleaned)\n",
    "    return docs_cleaned\n",
    "emails_cleaned= clean_text(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 265)\t2\n",
      "  (0, 482)\t2\n",
      "  (0, 363)\t2\n",
      "  (0, 229)\t1\n",
      "  (0, 361)\t2\n",
      "  (0, 103)\t1\n",
      "  (0, 997)\t1\n",
      "  (0, 86)\t2\n",
      "  (0, 72)\t1\n",
      "  (0, 476)\t1\n",
      "  (0, 897)\t1\n",
      "  (0, 691)\t1\n",
      "  (0, 506)\t1\n",
      "  (0, 864)\t1\n",
      "  (0, 585)\t1\n",
      "  (0, 151)\t1\n",
      "  (0, 715)\t1\n",
      "  (0, 968)\t1\n",
      "  (0, 932)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\", max_features=1000, max_df=0.5, min_df=2)\n",
    "docs_cv = cv.fit_transform(emails_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate first doc\n",
    "sparse vector is in the form of the following:\n",
    "(row index, term index) term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 265)\t2\n",
      "  (0, 482)\t2\n",
      "  (0, 363)\t2\n",
      "  (0, 229)\t1\n",
      "  (0, 361)\t2\n",
      "  (0, 103)\t1\n",
      "  (0, 997)\t1\n",
      "  (0, 86)\t2\n",
      "  (0, 72)\t1\n",
      "  (0, 476)\t1\n",
      "  (0, 897)\t1\n",
      "  (0, 691)\t1\n",
      "  (0, 506)\t1\n",
      "  (0, 864)\t1\n",
      "  (0, 585)\t1\n",
      "  (0, 151)\t1\n",
      "  (0, 715)\t1\n",
      "  (0, 968)\t1\n",
      "  (0, 932)\t1\n"
     ]
    }
   ],
   "source": [
    "print(docs_cv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding terms can be found through feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsubscribe\n",
      "website\n",
      "read\n"
     ]
    }
   ],
   "source": [
    "terms = cv.get_feature_names()\n",
    "print(terms[932])\n",
    "print(terms[968])\n",
    "print(terms[715])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by label and record the index of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_index(labels):\n",
    "    from collections import defaultdict\n",
    "    label_index = defaultdict(list)\n",
    "    for index, label in enumerate(labels):\n",
    "        label_index[label].append(index)\n",
    "    return label_index\n",
    "label_index = get_label_index(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: {1: 0.2900232018561485, 0: 0.7099767981438515}\n"
     ]
    }
   ],
   "source": [
    "def get_prior(label_index):\n",
    "    \"\"\"\n",
    "    Compute prior based on training samples\n",
    "    @param label_index: grouped sample indices by class\n",
    "    @return: dictionary, with class label as key, corresponding prior as the value\n",
    "    \"\"\"\n",
    "    prior = {label: len(index) for label, index in label_index.items()}\n",
    "    total_count = sum(prior.values())\n",
    "    for label in prior:\n",
    "        prior[label] /= float(total_count)\n",
    "    return prior\n",
    "prior = get_prior(label_index)\n",
    "print(\"Prior:\", prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
