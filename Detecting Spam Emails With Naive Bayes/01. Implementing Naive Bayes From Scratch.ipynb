{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a Ham Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "corpus_path = 'C:/Users/Rik/Documents/corpus/'\n",
    "file_path = corpus_path + 'enron1/ham/0007.1999-12-14.farmer.ham.txt'\n",
    "with open(file_path, 'r') as infile:\n",
    "    ham_sample = infile.read()\n",
    "    print(ham_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a Spam Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: stacey automated system generating 8 k per week parallelogram\n",
      "people are\n",
      "getting rich using this system ! now it ' s your\n",
      "turn !\n",
      "we ' ve\n",
      "cracked the code and will show you . . . .\n",
      "this is the\n",
      "only system that does everything for you , so you can make\n",
      "money\n",
      ". . . . . . . .\n",
      "because your\n",
      "success is . . . completely automated !\n",
      "let me show\n",
      "you how !\n",
      "click\n",
      "here\n",
      "to opt out click here % random _ text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = corpus_path + 'enron1/spam/0058.2003-12-21.GP.spam.txt'\n",
    "with open(file_path, 'r') as infile:\n",
    "    spam_sample = infile.read()\n",
    "    print(spam_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in ham and spam email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails, labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFilesForFolder(folder_path, is_spam):\n",
    "    file_path = corpus_path + 'enron1/' + folder_path\n",
    "    for filename in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding=\"ISO-8859-1\") as infile:\n",
    "            emails.append(infile.read())\n",
    "            labels.append(is_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "1500\n",
      "5172\n",
      "5172\n"
     ]
    }
   ],
   "source": [
    "readFilesForFolder('spam', 1)\n",
    "print(len(emails))\n",
    "print(len(labels))\n",
    "readFilesForFolder('ham', 0)\n",
    "print(len(emails))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess and clean raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def is_letter_only(word):\n",
    "    return word.isalpha()\n",
    "all_names = set(names.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def clean_text(docs):\n",
    "    docs_cleaned = []\n",
    "    for doc in docs:\n",
    "        doc = doc.lower()\n",
    "        doc_cleaned = ' '.join(lemmatizer.lemmatize(word) for word in doc.split() if is_letter_only(word))\n",
    "        docs_cleaned.append(doc_cleaned)\n",
    "    return docs_cleaned\n",
    "emails_cleaned= clean_text(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 265)\t2\n",
      "  (0, 482)\t2\n",
      "  (0, 363)\t2\n",
      "  (0, 229)\t1\n",
      "  (0, 361)\t2\n",
      "  (0, 103)\t1\n",
      "  (0, 997)\t1\n",
      "  (0, 86)\t2\n",
      "  (0, 72)\t1\n",
      "  (0, 476)\t1\n",
      "  (0, 897)\t1\n",
      "  (0, 691)\t1\n",
      "  (0, 506)\t1\n",
      "  (0, 864)\t1\n",
      "  (0, 585)\t1\n",
      "  (0, 151)\t1\n",
      "  (0, 715)\t1\n",
      "  (0, 968)\t1\n",
      "  (0, 932)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\", max_features=1000, max_df=0.5, min_df=2)\n",
    "docs_cv = cv.fit_transform(emails_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate first doc\n",
    "sparse vector is in the form of the following:\n",
    "(row index, term index) term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 265)\t2\n",
      "  (0, 482)\t2\n",
      "  (0, 363)\t2\n",
      "  (0, 229)\t1\n",
      "  (0, 361)\t2\n",
      "  (0, 103)\t1\n",
      "  (0, 997)\t1\n",
      "  (0, 86)\t2\n",
      "  (0, 72)\t1\n",
      "  (0, 476)\t1\n",
      "  (0, 897)\t1\n",
      "  (0, 691)\t1\n",
      "  (0, 506)\t1\n",
      "  (0, 864)\t1\n",
      "  (0, 585)\t1\n",
      "  (0, 151)\t1\n",
      "  (0, 715)\t1\n",
      "  (0, 968)\t1\n",
      "  (0, 932)\t1\n"
     ]
    }
   ],
   "source": [
    "print(docs_cv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding terms can be found through feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsubscribe\n",
      "website\n",
      "read\n"
     ]
    }
   ],
   "source": [
    "terms = cv.get_feature_names()\n",
    "print(terms[932])\n",
    "print(terms[968])\n",
    "print(terms[715])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by label and record the index of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_index(labels):\n",
    "    from collections import defaultdict\n",
    "    label_index = defaultdict(list)\n",
    "    for index, label in enumerate(labels):\n",
    "        label_index[label].append(index)\n",
    "    return label_index\n",
    "label_index = get_label_index(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: {1: 0.2900232018561485, 0: 0.7099767981438515}\n"
     ]
    }
   ],
   "source": [
    "def get_prior(label_index):\n",
    "    \"\"\"\n",
    "    Compute prior based on training samples\n",
    "    @param label_index: grouped sample indices by class\n",
    "    @return: dictionary, with class label as key, corresponding prior as the value\n",
    "    \"\"\"\n",
    "    prior = {label: len(index) for label, index in label_index.items()}\n",
    "    total_count = sum(prior.values())\n",
    "    for label in prior:\n",
    "        prior[label] /= float(total_count)\n",
    "    return prior\n",
    "prior = get_prior(label_index)\n",
    "print(\"Prior:\", prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_likelihood(term_matrix, label_index, smoothing=0):\n",
    "    \"\"\"\n",
    "    Compute likelihood based on training samples\n",
    "    @param term_matrix: sparse matrix of the term frequency features\n",
    "    @param label_index: grouped sample indices by class\n",
    "    @param smoothing: integer, additive Laplace smoothing parameter\n",
    "    @return: dictionary, with class as key, corresponding conditional probability P(feature|class) vector as value\n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    for label, index in label_index.items():\n",
    "        likelihood[label] = term_matrix[index, :].sum(axis=0) + smoothing\n",
    "        likelihood[label] = np.asarray(likelihood[label])[0]\n",
    "        total_count = likelihood[label].sum()\n",
    "        likelihood[label] = likelihood[label] / float(total_count)\n",
    "    return likelihood\n",
    "smoothing = 1\n",
    "likelihood = get_likelihood(docs_cv, label_index, smoothing)\n",
    "len(likelihood[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00024653 0.00090705 0.00080007 0.00032096 0.00073495]\n",
      "[0.00063304 0.00078026 0.00101581 0.00022083 0.00326826]\n"
     ]
    }
   ],
   "source": [
    "print(likelihood[0][:5])\n",
    "print(likelihood[1][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(term_matrix, prior, likelihood):\n",
    "    \"\"\"\n",
    "    Compute posterior of testing samples, based on prior and likelihood\n",
    "    @param term_matrix: sparse matrix of the term frequency features\n",
    "    @param prior: dictionary, with class label as key,\n",
    "                    corresponding to prior as the value\n",
    "    @param likelihood: dictionary, with class label as key,\n",
    "                    corresponding conditional probability vector as value\n",
    "    @return: dictionary, with class label as key, corresponding posterior as value\n",
    "    \"\"\"\n",
    "    num_docs = term_matrix.shape[0]\n",
    "    posteriors = []\n",
    "    for i in range(num_docs):\n",
    "        # posterior is proportional to prior * likelihood\n",
    "        # = exp(log(prior * likelihood))\n",
    "        # = exp(log(prior) + log(likelihood))\n",
    "        posterior = { key: np.log(prior_label) for key, prior_label in prior.items() }\n",
    "        for label, likelihood_label in likelihood.items():\n",
    "            term_document_vector = term_matrix.getrow(i)\n",
    "            counts = term_document_vector.data\n",
    "            indices = term_document_vector.indices\n",
    "            for count, index in zip(counts, indices):\n",
    "                posterior[label] += np.log(likelihood_label[index]) * count\n",
    "        # exp(-1000):exp(-999) will cause division error,\n",
    "        # however it equates to exp(0):exp(1)\n",
    "        min_log_posterior = min(posterior.values())\n",
    "        for label in posterior:\n",
    "            try:\n",
    "                posterior[label] = np.exp(posterior[label] - min_log_posterior)\n",
    "            except:\n",
    "                posterior[label] = float('inf')\n",
    "        # normalize so that all sums up to 1\n",
    "        sum_posterior = sum(posterior.values())\n",
    "        for label in posterior:\n",
    "            if posterior[label] == float('inf'):\n",
    "                posterior[label] = 1.0\n",
    "            else:\n",
    "                posterior[label] /= sum_posterior\n",
    "        posteriors.append(posterior.copy())\n",
    "    return posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_test = [\n",
    "    '''Subject: flat screens\n",
    "    hello,\n",
    "    please call or contact regarding the other flat screens\n",
    "    requested .\n",
    "    trisha tlapek - eb 3132 b\n",
    "    michael sergeev - eb 3132 a\n",
    "    also the sub blocker that was taken away from eb 3131 a .\n",
    "    trisha should two monitors also michael .\n",
    "    thanks\n",
    "    kevin moore''',\n",
    "    '''Subject: let ' s stop the mlm insanity !\n",
    "    still believe you can earn $ 100 , 000 fast in mlm ? get real !\n",
    "    get emm , a brand new system that replaces mlm with something that works !\n",
    "    start earning 1 , 000 ' s now ! up to $ 10 , 000 per week doing simple online tasks .\n",
    "    free info - breakfree @ luxmail . com - type \" send emm info \" in the\n",
    "    subject box .\n",
    "    this message is sent in compliance of the proposed bill section 301 . per\n",
    "    section 301 , paragraph ( a ) ( 2 ) ( c ) of s . 1618 . further transmission\n",
    "    to you by the sender of this e - mail may be stopped at no cost to you by\n",
    "    sending a reply to : \" email address \" with the word remove in the line .'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1: 3.47152510287432e-09, 0: 0.9999999965284749}, {1: 0.999999999999999, 0: 9.596751608722133e-16}]\n"
     ]
    }
   ],
   "source": [
    "emails_cleaned_test = clean_text(emails_test)\n",
    "term_docs_test = cv.transform(emails_cleaned_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3465 3465\n",
      "1707 1707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(emails_cleaned, labels, test_size=0.33, random_state=42)\n",
    "print(len(X_train), len(Y_train))\n",
    "print(len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train just training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_docs_train = cv.fit_transform(X_train)\n",
    "label_index = get_label_index(Y_train)\n",
    "prior = get_prior(label_index)\n",
    "likelihood = get_likelihood(term_docs_train, label_index, smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict posterior for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "term_docs_test = cv.transform(X_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on 1707 testing samples is : 93.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0.0\n",
    "for pred, actual in zip(posterior, Y_test):\n",
    "    if actual == 1:\n",
    "        if pred[1] >= 0.5:\n",
    "            correct += 1\n",
    "    elif pred[0] > 0.5:\n",
    "        correct += 1\n",
    "print('The accuracy on {0} testing samples is : {1:.1f}%'.format(len(Y_test), correct/len(Y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
